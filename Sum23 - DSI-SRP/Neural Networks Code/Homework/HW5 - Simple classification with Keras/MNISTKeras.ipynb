{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# supress unnecessary warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## MINST happens to be preloaded with Keras\n",
    "##\n",
    "\n",
    "# load mnist\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# display some digits\n",
    "fig = plt.figure()\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(train_images[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Digit: {}\".format(train_labels[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# image shape\n",
    "sz = train_images.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## preprocess training and testing patterns\n",
    "##\n",
    "## this is a common first step for training any neural network\n",
    "##\n",
    "\n",
    "# check out dimensions and types of mnist data\n",
    "print('*** Original Training and Testing Patterns ***')\n",
    "print('Training images shape: ', train_images.shape)\n",
    "print('Training images type:  ', type(train_images[0][0][0]))\n",
    "print('Testing images shape:  ', test_images.shape)\n",
    "print('Testing images type:   ', type(test_images[0][0][0]))\n",
    "print()\n",
    "\n",
    "# need to reshape and preprocess the training/testing images\n",
    "train_images_vec = train_images.reshape((train_images.shape[0], train_images.shape[1] * train_images.shape[2]))\n",
    "train_images_vec = train_images_vec.astype('float32') / 255\n",
    "test_images_vec = test_images.reshape((test_images.shape[0], test_images.shape[1] * test_images.shape[2]))\n",
    "test_images_vec = test_images_vec.astype('float32') / 255\n",
    "\n",
    "# display new input dimensions/type\n",
    "print('*** Reformatted Training and Testing Patterns ***')\n",
    "print('Training images shape: ', train_images_vec.shape)\n",
    "print('Training images type:  ', type(train_images_vec[0][0]))\n",
    "print('Testing images shape:  ', test_images_vec.shape)\n",
    "print('Testing images type:   ', type(test_images_vec[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## MINST labels are numeric - want to reformat as one-hot coded vectors\n",
    "##\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# also need to categorically encode the labels\n",
    "print('First 5 training labels as labels:\\n', train_labels[:5])\n",
    "\n",
    "train_labels_onehot = to_categorical(train_labels)\n",
    "test_labels_onehot = to_categorical(test_labels)\n",
    "print('First 5 training labels as one-hot encoded vectors:\\n', train_labels_onehot[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start with a simple one-layer neural network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "##\n",
    "## define and train neural network in Keras\n",
    "##\n",
    "\n",
    "# import tools for basic keras networks \n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers# number of input node (nin = 784)\n",
    "nin = train_images_vec.shape[1]\n",
    "\n",
    "# number of output nodes (nout = 10)\n",
    "nout = train_labels_onehot.shape[1]\n",
    "\n",
    "# create architecture of simple neural network model\n",
    "# input layer  : 28*28 = 784 (nin) input nodes\n",
    "# output layer : 10 (nout) sigmoid output nodes\n",
    "\n",
    "# Sequential is a basic stack of layers (most basic type of neural network)\n",
    "# https://keras.io/getting-started/sequential-model-guide/\n",
    "#\n",
    "# this initializes a blank Sequential network\n",
    "network = models.Sequential()\n",
    "\n",
    "# network.add() adds the first (and only) layer to the network (the output layer)\n",
    "# \n",
    "# layers.Dense() is a densely connect layer\n",
    "# with nout units\n",
    "# with a sigmoidal activation function\n",
    "# that receives input from an input later with a specific shape\n",
    "# \n",
    "network.add(layers.Dense(nout, \n",
    "                         activation='sigmoid', \n",
    "                         input_shape=(nin,)))\n",
    "\n",
    "# what happens if nin isn't included on first layer?\n",
    "#network.add(layers.Dense(nout, \n",
    "#                         activation='sigmoid'))\n",
    "\n",
    "# with multi-layer networks, we will have additional network.add() calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a model summary\n",
    "print(network.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print more info about the network\n",
    "\n",
    "print('*** Basic Network Structure ***')\n",
    "for layer in network.layers:\n",
    "    print('layer name : {} | input shape : {} | output shape : {}'.format(layer.name, layer.input.shape, layer.output.shape))\n",
    "print()\n",
    "print('*** Detailed Network Layer Information ***')\n",
    "for layer in network.layers:\n",
    "    print(layer.get_config())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that before training, weights are initialized to small random values\n",
    "print('W shape : {}'.format(network.layers[0].get_weights()[0].shape))\n",
    "print('weight (W) initial values:')\n",
    "print(network.layers[0].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "#\n",
    "# configures the network for training, specifying optimizer and loss function\n",
    "#\n",
    "# optimizer='sgd'           : stochastic gradient descent (simplest, not the smartest)\n",
    "# loss='mean_squared_error' : uses MSE (MSE = 1/N * SSE)\n",
    "# metrics=['accuracy']      : what is printed when verbose=True\n",
    "network.compile(optimizer='sgd', \n",
    "                loss='mean_squared_error', \n",
    "                metrics=['accuracy', 'mse'])\n",
    "\n",
    "# if leave off metrics, only saves loss and val_loss\n",
    "# network.compile(optimizer='sgd', \n",
    "#                 loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that before training, weights are initialized to small random values\n",
    "print('W shape : {}'.format(network.layers[0].get_weights()[0].shape))\n",
    "print('weight (W) initial values:')\n",
    "print(network.layers[0].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now train the network\n",
    "#\n",
    "# training requires training patterns (train_image_vec) and teachers (train_labels_onehot)\n",
    "#\n",
    "# sets # training epochs, validation (described later), and batch_size\n",
    "#\n",
    "# set verbose=True to see training unfold\n",
    "history = network.fit(train_images_vec, \n",
    "                      train_labels_onehot, \n",
    "                      verbose=True, \n",
    "                      validation_split=.1, \n",
    "                      epochs=20, \n",
    "                      batch_size=128)\n",
    "print('Done training!')\n",
    "\n",
    "# if run this again, it will do more training on the same network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history contains the loss (training loss), accuracy, mse, val_loss (validation loss), \n",
    "# val_accuracy (validation accuracy), val_mse, as a Python dictionary\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access one of the elements of the dictionary by name (in this case training accuracy)\n",
    "print(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call network.evaluate() if you have test patterns and test answers and want to know performance\n",
    "test_results = network.evaluate(test_images_vec, \n",
    "                                test_labels_onehot, \n",
    "                                verbose=False)\n",
    "\n",
    "# loss is MSE\n",
    "# accuracy is proportion correct\n",
    "print(network.metrics_names)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call network.predict() if you have test patterns and want to get predicted outputs\n",
    "out = network.predict(test_images_vec)\n",
    "\n",
    "print('dimensions of out : {}'.format(out.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi-layer neural network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# number of input node (nin = 784)\n",
    "nin = train_images_vec.shape[1]\n",
    "\n",
    "# create a multi-layer network with two layers of nhid hidden nodes\n",
    "nhid = 100\n",
    "\n",
    "# number of output nodes (nout = 10)\n",
    "nout = train_labels_onehot.shape[1]\n",
    "\n",
    "# create architecture of multi-layer neural network model\n",
    "# input layer  : 28*28 = 784 (nin) input nodes\n",
    "# hidden layer : 100 hidden nodes\n",
    "# output layer : 10 (nout) softmax output nodes\n",
    "\n",
    "# this initializes a blank Sequential network\n",
    "network2 = models.Sequential()\n",
    "\n",
    "# add layers to the initialized network\n",
    "# \n",
    "# hidden layer (input->hidden) - using relu because of its nice mathematical properties\n",
    "network2.add(layers.Dense(nhid, \n",
    "                          kernel_regularizer=regularizers.l2(0.01), \n",
    "                          activation='relu', \n",
    "                          input_shape=(nin,)))\n",
    "\n",
    "# hidden layer (hidden->hidden) - using relu because of its nice mathematical properties\n",
    "network2.add(layers.Dense(nhid, \n",
    "                          kernel_regularizer=regularizers.l2(0.01), \n",
    "                          activation='relu'))\n",
    "\n",
    "# output layer (hidden->output) - using softmax as per discussion in class\n",
    "network2.add(layers.Dense(nout, \n",
    "                          kernel_regularizer=regularizers.l2(0.01), \n",
    "                          activation='softmax'))\n",
    "\n",
    "# regularizers (L1 or L2) can potentially help with over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a model summary\n",
    "print(network2.summary())\n",
    "\n",
    "# print more info about the network\n",
    "\n",
    "print('*** Basic Network Structure ***')\n",
    "for layer in network2.layers:\n",
    "    print('layer name : {} | input shape : {} | output shape : {}'.format(layer.name, layer.input.shape, layer.output.shape))\n",
    "print()\n",
    "print('*** Detailed Network Layer Information ***')\n",
    "for layer in network2.layers:\n",
    "    print(layer.get_config())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "#\n",
    "# using 'adam' optimizer (extension of stochastic gradient descent)\n",
    "#\n",
    "# using categorical cross entropy with softmax output activation\n",
    "network2.compile(optimizer='adam', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now train the network\n",
    "#\n",
    "# validation is used to adjust optimization\n",
    "history = network2.fit(train_images_vec, \n",
    "                       train_labels_onehot, \n",
    "                       verbose=True, \n",
    "                       validation_split=.1, \n",
    "                       epochs=20, \n",
    "                       batch_size=128)\n",
    "print('Done training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4e8bc7389cfca2533becb725d38fc3a7d96e5f0f9dc1d3a6cde80de95c41b25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

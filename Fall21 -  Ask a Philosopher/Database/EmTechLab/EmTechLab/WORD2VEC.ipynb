{"cells":[{"cell_type":"markdown","metadata":{"id":"a6tp4sYlNGt5"},"source":["# word2vec\n","In this notebook, we will:\n","> 1. explore different pretrained word2vec models offered by Gensim (\"**Gen**erate **Sim**ilar\") python library\n","\n","\n","> 2. create a function that accepts a string, and returns a vector representation of that string\n","\n","https://intellica-ai.medium.com/comparison-of-different-word-embeddings-on-text-similarity-a-use-case-in-nlp-e83e08469c1c "]},{"cell_type":"markdown","metadata":{"id":"bSu99DbGPhp1"},"source":["**Installing Dependencies**"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":3435,"status":"ok","timestamp":1660542893953,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"},"user_tz":240},"id":"Q1o7vF74Pgwm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2f90f48e-4d13-4bd4-eca0-373fdbedc423"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.4)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}],"source":["import gensim.downloader as api # using gensim api to download models and datasets\n","import numpy as np # numerical python to work with word embeddings\n","\n","# For preprocessing text\n","import re \n","!pip install unidecode\n","from unidecode import unidecode\n","\n","# Natural language toolkit functions and datasets\n","import nltk\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords \n","from nltk.stem import WordNetLemmatizer # WordNet is \n","from nltk.stem.snowball import SnowballStemmer\n","nltk.download('punkt') \n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","from pprint import pprint # to print dictionaries nicely"]},{"cell_type":"markdown","metadata":{"id":"bWJX_LFcN6nq"},"source":["#### **1. Exploring models**"]},{"cell_type":"markdown","metadata":{"id":"5rpGxwEJOHPn"},"source":["Gensim comes with several already pre-trained models, in their [Gensim-data repository](https://github.com/RaRe-Technologies/gensim-data) on Github. Let's load in in and take a look at a few. \n","We will be looking to load or train a model with a:\n","* small distance window\n","* high dimensionality"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"i_YKkVpTPXvt","executionInfo":{"status":"ok","timestamp":1660538301872,"user_tz":240,"elapsed":169,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}}},"outputs":[],"source":["%%capture \n","# Let's look at the models and datasets available for use (comment out the capture to see JSON)\n","\n","# ------- 'corpora' are first half (key in JSON)\n","# -------- 'models' are second half (key in JSON) \n","\n","info = api.info()\n","pprint(info)"]},{"cell_type":"markdown","metadata":{"id":"blfBK1uEPI6T"},"source":["**Here are two interesting models:**\n","\n","1. *word2vec-google-news-300:*\n","\n","* 'description': Google News (about 100 billion words)\n"," * readmore: https://code.google.com/archive/p/word2vec/ \n","\n","2. *glove-wiki-gigaword-300* \n","* Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\n"," * readmore:https://nlp.stanford.edu/projects/glove/ "]},{"cell_type":"markdown","source":["**glove-wiki-gigaword-50** (word2vec model)"],"metadata":{"id":"UjPomrpniwHR"}},{"cell_type":"code","source":["# Loading the model in\n","model_glove_50 = api.load(\"glove-wiki-gigaword-50\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BE_eeb4Hi2FD","executionInfo":{"status":"ok","timestamp":1660538357424,"user_tz":240,"elapsed":55554,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}},"outputId":"aaa7171b-b420-48fe-f99c-737ba787c36d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 66.0/66.0MB downloaded\n"]}]},{"cell_type":"code","source":["# Grabbing the dictionary (key-vector mapping class) of word embeddings (semantic vectors)\n","glove_vectors = model_glove_50.wv\n","print(type(glove_vectors))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UYlMLhnji-HR","executionInfo":{"status":"ok","timestamp":1660538357425,"user_tz":240,"elapsed":8,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}},"outputId":"8fd8b6c1-7748-435a-b76d-c6dfb0cd69c9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  \n"]}]},{"cell_type":"code","source":["# Grabbing at most similar semantic vectors to freedom\n","test_word = \"freedom\"\n","freedom = glove_vectors.most_similar(test_word)\n","\n","# Priniting word embedding for a word\n","glove_vectors[test_word]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YaBv4XyijXE2","executionInfo":{"status":"ok","timestamp":1660538893498,"user_tz":240,"elapsed":115,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}},"outputId":"c217f136-6fae-47bd-a02d-24bd53fb8b38"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.75489  ,  0.27814  , -0.11302  , -0.37057  ,  0.57873  ,\n","       -0.14529  ,  0.43383  , -0.45607  ,  0.7262   ,  0.31185  ,\n","        0.24079  , -0.11883  , -0.20353  , -0.29989  , -0.27301  ,\n","       -0.23686  ,  0.51582  , -0.47091  ,  0.31237  ,  0.0070554,\n","       -0.22833  ,  0.89127  , -0.32475  , -0.22581  ,  0.43705  ,\n","       -1.6628   , -0.64576  , -1.0098   ,  0.37792  ,  0.33504  ,\n","        2.6654   ,  0.58524  , -1.3425   , -0.40824  , -1.4958   ,\n","       -0.64544  ,  0.071664 , -0.80439  , -0.76056  ,  0.36512  ,\n","        0.32903  , -0.25687  ,  0.13765  ,  0.39533  , -0.68773  ,\n","       -0.043908 , -0.95513  , -0.47569  , -0.33671  , -0.44242  ],\n","      dtype=float32)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# Performing vector addition to explore relationships\n","\n","def magnitude(a):\n","  return np.linalg.norm(a)\n","\n","# Reference \n","ref = glove_vectors['icecream'] - glove_vectors['vanilla']  - (glove_vectors['pizza'] - glove_vectors['sauce'])\n","\n","# KING is to MAN as QUEEN is to WOMAN\n","test1 = glove_vectors['king'] - glove_vectors['man']  - (glove_vectors['queen'] - glove_vectors['woman'])\n","\n","# ACTOR is to MAN as ACTRESS is to WOMAN\n","test2 = glove_vectors['actor'] - glove_vectors['man']  - (glove_vectors['actress'] - glove_vectors['woman'])\n","\n","\n","print(\"Reference magnitude:\", magnitude(ref))\n","print(\"Test 1 magnitude:\", magnitude(test1))\n","print(\"Test 2 magnitude:\", magnitude(test2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lT1ZX3nkQFtK","executionInfo":{"status":"ok","timestamp":1660538357617,"user_tz":240,"elapsed":4,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}},"outputId":"8e59a2fa-0d30-4de8-bd52-58845c85481a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Reference magnitude: 6.263511\n","Test 1 magnitude: 2.8391206\n","Test 2 magnitude: 2.0526736\n"]}]},{"cell_type":"markdown","metadata":{"id":"t2wDgh9gUKWK"},"source":["#### **2. Defining function**"]},{"cell_type":"markdown","metadata":{"id":"zikQbZcIU4UF"},"source":["In this section, we will write a function that:\n","1. preprocesses a string (removing stopwords, punctuation, and uppercase letters)\n","2. associates a vector with the preprocessed strings"]},{"cell_type":"markdown","source":["###### **Preprocessing**"],"metadata":{"id":"jZJOcMRcy8Xe"}},{"cell_type":"code","execution_count":38,"metadata":{"id":"eBtEWdomURXc","executionInfo":{"status":"ok","timestamp":1660542899900,"user_tz":240,"elapsed":139,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}}},"outputs":[],"source":["example_string = \"नमस्कार THERE, this is 1 notebook aimed at implementing part of a doc2vec function!!!\""]},{"cell_type":"code","execution_count":39,"metadata":{"id":"lDltnY2vV_cc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660542900390,"user_tz":240,"elapsed":3,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}},"outputId":"27891c53-8c84-49df-9d03-85c6c0996242"},"outputs":[{"output_type":"stream","name":"stdout","text":["Example string 0.0: नमस्कार THERE, this is 1 notebook aimed at implementing part of a doc2vec function!!!\n","Example string 1.0: नमस्कार there, this is 1 notebook aimed at implementing part of a doc2vec function!!!\n","Example string 2.0:         there  this is   notebook aimed at implementing part of a doc vec function   \n","Example string 3.0:  there this is notebook aimed at implementing part of a doc vec function \n"]}],"source":["# preprocessing the string\n","\n","print(\"Example string 0.0:\", example_string)\n","\n","# making words lowercase\n","example_string = example_string.lower()\n","print(\"Example string 1.0:\", example_string)\n","\n","# removing all digits, punctuation, special characters\n","example_string = re.sub('[^a-zA-Z]', ' ', example_string) \n","examle_string = unidecode(example_string)\n","print(\"Example string 2.0:\", example_string)\n","\n","# Removing whitespace\n","example_string = re.sub(r'\\s+', ' ', example_string)\n","print(\"Example string 3.0:\", example_string) "]},{"cell_type":"markdown","metadata":{"id":"LnkpEJBdWaT_"},"source":["That's string is looking much better! Three more things we need to do in this preprocessing stage is:\n","> 1. Splitting sentence into word\n","> 2. Stemming and lemmatizing each word\n","> 3. Remove stopwords"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"K-s9tpoaW7X5","executionInfo":{"status":"ok","timestamp":1660542902228,"user_tz":240,"elapsed":122,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}}},"outputs":[],"source":["# Tokenizing the sentence\n","words = word_tokenize(example_string)\n","\n","# Removing all stopwords, stemming, and lemmatizing\n","lemmatizer = WordNetLemmatizer()\n","ss = SnowballStemmer('english')\n","\n","example_sentence = [lemmatizer.lemmatize(ss.stem(w)) for w in words if w not in stopwords.words('english')]"]},{"cell_type":"code","source":["example_sentence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V3pv5tApiLOL","executionInfo":{"status":"ok","timestamp":1660542949920,"user_tz":240,"elapsed":118,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}},"outputId":"efb64d68-fef6-4d87-e147-0f936541cc10"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['notebook', 'aim', 'implement', 'part', 'doc', 'vec', 'function']"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","source":["###### **Sent2Vec Function**"],"metadata":{"id":"NAiRIWFyzMkk"}},{"cell_type":"code","source":["# Creating Euclidean Distance based function\n","\n","def sentence_vector(sentence):\n","  \"\"\"Add up all the semantic vectors for each sentence to get a sentence vector.\"\"\"\n","  s_vector = np.array(50*[0])\n","\n","  for word in sentence:\n","    try:\n","      word_vector = glove_vectors[word]\n","      s_vector = word_vector\n","    except:\n","      pass\n","\n","  return s_vector"],"metadata":{"id":"INJnk7ulzokX","executionInfo":{"status":"ok","timestamp":1660539433490,"user_tz":240,"elapsed":114,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["sentence_vector([\"man\", \"sad\", \"at\", \"bar\", \"had\", \"cups\", \"whiskey\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mtkzdpzm1wad","executionInfo":{"status":"ok","timestamp":1660539434668,"user_tz":240,"elapsed":117,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}},"outputId":"4f13faeb-bc1d-425c-f74b-9aff3eb1141b"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-1.4145   , -0.056566 , -0.73252  , -0.60502  , -0.018337 ,\n","        0.52306  , -0.49091  ,  0.49644  ,  0.2371   ,  1.4823   ,\n","        0.0066497,  0.43641  ,  0.53346  , -0.52744  ,  0.26829  ,\n","       -0.1701   ,  0.35027  ,  0.55006  , -0.59988  , -1.1519   ,\n","        1.0168   , -0.51179  ,  0.73453  ,  0.56626  , -0.71344  ,\n","       -1.0084   , -0.58305  ,  0.68873  ,  1.4397   ,  0.0021525,\n","        0.31099  , -0.65845  , -0.53219  ,  1.4891   ,  1.1693   ,\n","       -0.50109  , -0.60882  , -0.32086  ,  0.37159  ,  0.37465  ,\n","        0.54943  ,  0.39997  , -0.41693  ,  0.026556 ,  0.20353  ,\n","        0.23431  , -0.36537  , -0.70014  , -0.050047 , -0.95843  ],\n","      dtype=float32)"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["sentence_vector([\"guy\", \"mad\", \"icecream\", \"eating\", \"resteraunt\", \"wine\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L8pXvllx174y","executionInfo":{"status":"ok","timestamp":1660539436152,"user_tz":240,"elapsed":139,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}},"outputId":"e3fae619-7a66-4442-e581-3a4ebac072ee"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.1145  ,  0.75404 , -1.6432  , -0.61038 ,  0.60352 , -0.56396 ,\n","       -1.0069  , -0.44103 ,  0.61256 ,  1.1812  ,  0.18128 ,  0.30032 ,\n","        1.1817  , -0.62548 ,  1.2156  , -0.30738 ,  0.54095 ,  0.53758 ,\n","       -0.026086, -1.7387  ,  0.46533 , -0.62835 ,  0.50936 ,  1.1192  ,\n","       -0.74747 , -0.57528 , -0.9203  ,  0.98612 ,  0.29107 ,  0.60208 ,\n","        1.9703  , -0.27461 , -0.34921 ,  0.44141 ,  0.64402 , -0.32353 ,\n","       -1.4541  ,  1.1472  ,  0.86875 , -0.074512,  0.85632 ,  0.59341 ,\n","        0.4655  , -0.0387  ,  0.26463 ,  0.94151 , -0.27335 , -0.085403,\n","        0.12693 , -0.23861 ], dtype=float32)"]},"metadata":{},"execution_count":35}]}],"metadata":{"colab":{"name":"WORD2VEC.ipynb","provenance":[],"collapsed_sections":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Quotes_df.ipynb","provenance":[],"authorship_tag":"ABX9TyNZw5cl/7CUG5EdIpLzYitj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Collecting Quotes\n","In this notebook, we will be creating a dataframe with two columns:\n","1. Text - Full or abbr. quote of Krishnamurti\n","2. URL - Link to full or find the quote\n","\n","Here are the website we will be scraping to create our dataframe:\n","1. [Good Reads](https://www.goodreads.com/author/quotes/850512.J_Krishnamurti?page=1)\n","2. [Brainy Quotes](https://www.brainyquote.com/authors/jiddu-krishnamurti-quotes)\n","3.  [wikiquotes](https://en.wikiquote.org/wiki/Jiddu_Krishnamurti)\n","4.  [jkrishnamurti.org](https://jkrishnamurti.org/jksearch?keyword=&page=1&type=16618)\n"],"metadata":{"id":"uk9kIcZEq7fK"}},{"cell_type":"markdown","source":["**Installing Dependencies**"],"metadata":{"id":"qe5kpRYbGQxr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ShoESjnuq4uK"},"outputs":[],"source":["import requests                     # To make 'get' requests through chrome browser\n","from bs4 import BeautifulSoup       # To parse html file in python tree object\n","import pandas as pd                 # To create Dataframe and save data into JSON file\n","\n","import random\n","import time\n","from pprint import pprint\n","\n","from google.colab import data_table\n","from vega_datasets import data\n","data_table.enable_dataframe_formatter()"]},{"cell_type":"markdown","source":["**Getting the Hyperinks**"],"metadata":{"id":"bZyaTcXJGLSN"}},{"cell_type":"markdown","source":["In this section, we will:\n","1. Get all of the pages of quotes from goodreads\n","2. Associate each quote with a hyperlink\n","3. Create a dataframe from (quote, hyperlink pairs)\n"],"metadata":{"id":"mMMzCsbdyPBn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vpJOzuXFu3U"},"outputs":[],"source":["%%capture\n","# There are 33 pages of quotes we want to collect from\n","urls = []\n","base_url = \"https://www.goodreads.com/author/quotes/850512.J_Krishnamurti?page={}\"\n","search_pages = [base_url.format(str(i)) for i in range(1,34)]"]},{"cell_type":"markdown","source":["**Single Page**"],"metadata":{"id":"7fzJZip-ytMa"}},{"cell_type":"code","source":["plain_quotes = []\n","goodreads_quotes = []\n","\n","for search_page in search_pages:\n","  # Grabbing a list of quotes:\n","  quote_list = []\n","  page = requests.get(search_page)\n","  soup = BeautifulSoup(page.content, 'html.parser')\n","\n","  # Using CSS Selector to grab all quotes\n","  quotes = [quote.get_text() for quote in soup.find_all(\"div\", class_=\"quoteText\")]\n","  quotes = [quote.split(\"\\n\")[1].strip() for quote in quotes]\n","  plain_quotes.extend(quotes)\n","  quotes = [(quote, search_page) for quote in quotes]\n","\n","  goodreads_quotes.extend(quotes)"],"metadata":{"id":"dJyeRZQjwfEG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame.from_records(\n","    data=goodreads_quotes,\n","    columns=[\"Quote\", \"URL\"]\n",")"],"metadata":{"id":"olRSd4x_0J5p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df['Quote']\n","df.to_json('krish_quotes')"],"metadata":{"id":"i3RmFDluy5PD","executionInfo":{"status":"ok","timestamp":1660387399105,"user_tz":240,"elapsed":244,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}}},"execution_count":99,"outputs":[]},{"cell_type":"code","source":["%%capture\n","df.head(100)"],"metadata":{"id":"aUNBklIZ02Fq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Great!\n","Now that we have our quotes dataframe, we will associate each quote with a document centroid vector using word embeddings. The library we will be using for this word2vec task in **Gensim** (*gen*erate *sim*ilar), which has open-source software we can use to easily represent documents (or individual words) as vectors.\n","\n","*Fun fact: Gensim was a python project that arose in the effort to find the most similar mathematical articles to a given on in a Czech digital academic collection.*"],"metadata":{"id":"CIRhtaa3-mqT"}},{"cell_type":"code","source":["#Import all the dependencies\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from nltk.tokenize import word_tokenize"],"metadata":{"id":"fQqNHNoQSaqU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove quotations from plain quotes\n","plain_quotes = [quote[1:-1] for quote in plain_quotes]\n","plain_quotes[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"c13-1SSNTEO8","executionInfo":{"status":"ok","timestamp":1660379126761,"user_tz":240,"elapsed":250,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}},"outputId":"96919596-62ad-4ce9-9eb6-2bece640892c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'It is no measure of health to be well adjusted to a profoundly sick society.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["# Tegging list of quotes for gensim model\n","\n","tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(plain_quotes)]\n","pprint(tagged_data)"],"metadata":{"id":"YrEvRUdQSeP6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training gensim model\n","max_epochs = 100\n","vec_size = 20\n","alpha = 0.025\n","\n","model = Doc2Vec(size=vec_size,\n","                alpha=alpha, \n","                min_alpha=0.00025,\n","                min_count=1,\n","                dm =1)\n","  \n","model.build_vocab(tagged_data)\n","\n","for epoch in range(max_epochs):\n","    print('iteration {0}'.format(epoch))\n","    model.train(tagged_data,\n","                total_examples=model.corpus_count,\n","                epochs=model.iter)\n","    # decrease the learning rate\n","    model.alpha -= 0.0002\n","    # fix the learning rate, no decay\n","    model.min_alpha = model.alpha\n","\n","model.save(\"d2v.model\")\n","print(\"Model Saved\")"],"metadata":{"id":"0uIv6ADSTQsp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.models.doc2vec import Doc2Vec\n","\n","model= Doc2Vec.load(\"d2v.model\")\n","\n","# to find most similar doc using tags\n","similar_doc = model.docvecs.most_similar('1')\n","print(similar_doc)\n","\n","\n","# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n","print(model.docvecs['1'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3XYNYsXXgXQ","executionInfo":{"status":"ok","timestamp":1660380307665,"user_tz":240,"elapsed":228,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}},"outputId":"65dc2ded-f9c4-4c54-9b9f-6fb09be23956"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('970', 0.8046198487281799), ('47', 0.7607197761535645), ('965', 0.74647057056427), ('414', 0.7463820576667786), ('647', 0.7418216466903687), ('967', 0.7404366135597229), ('490', 0.735985279083252), ('56', 0.7330352067947388), ('275', 0.732215166091919), ('877', 0.7304850816726685)]\n","[-1.730693    1.5829514  -1.0686712   4.099093    0.5436246  -0.10553455\n","  1.5978084  -1.0038096  -1.149201    1.446069   -0.74242496  1.3126647\n","  4.3996644  -3.6409996  -1.6735785  -1.9980305   1.724739   -2.013659\n"," -1.1099366  -0.37500638]\n"]}]},{"cell_type":"markdown","source":["# Training a Doc2Vec Model\n"],"metadata":{"id":"tN1OR_rjcu0I"}},{"cell_type":"code","source":["# Installing dependencies\n","\n","# Importing generate similarities library for doc2vec\n","import gensim \n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"],"metadata":{"id":"oHgOdhUpd7mv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plain_quotes[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"DfqBwxZ-drl6","executionInfo":{"status":"ok","timestamp":1660381764319,"user_tz":240,"elapsed":286,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}},"outputId":"63609837-9edf-4e1f-e50c-02480d4731d1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'It is no measure of health to be well adjusted to a profoundly sick society.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":79}]},{"cell_type":"code","source":["# First we want to Tag and preprocess our quotes, so that they can be turned into vectors\n","\n","\"\"\"For each quote we create a TaggedDocument Object, with:\n","(1) words= Words tokens\n","(2) tag= index\"\"\"\n","train_corpus = [gensim.models.doc2vec.TaggedDocument(quote.lower().split(), [i]) for i, quote in enumerate(plain_quotes)]\n","train_corpus[0]"],"metadata":{"id":"q84uludGczdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","# Now we will create a gensim Doc2vec model\n","model = gensim.models.doc2vec.Doc2Vec(vector_size=50, # dimension of word embeddings\n","                                      min_count=2, # discard word with one or less occurence\n","                                      epochs= 40 # number of iterations over documents (diminishing returns)\n","                                      )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2NeZNUV_eSv3","executionInfo":{"status":"ok","timestamp":1660381969424,"user_tz":240,"elapsed":232,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}},"outputId":"0c70256b-8ac4-4671-bdab-8b1ca6182cfd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"]}]},{"cell_type":"code","source":["# Building vocabulary\n","model.build_vocab(train_corpus)"],"metadata":{"id":"8wXLXV9dekaV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training model\n","model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"],"metadata":{"id":"Mtzj5tpFfQ1N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using model to infer a vector, and compare to other vectors using cosine similarity\n","vector1 = model.infer_vector(\"The hardest problem in life the struggle\".lower().split())\n","vector2 = model.infer_vector(\"Love is for the kindess people\")\n","print(type(vector1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tH1DVHxRfXGb","executionInfo":{"status":"ok","timestamp":1660386919601,"user_tz":240,"elapsed":230,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}},"outputId":"fe053ab2-891e-4ec6-a912-e1456e6998e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'>\n"]}]},{"cell_type":"code","source":["from gensim.models import doc2vec\n","from scipy import spatial\n","\n","cos_distance = spatial.distance.cosine(vector1, vector2)\n","print(cos_distance)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0rJbbrcipGy","executionInfo":{"status":"ok","timestamp":1660386921301,"user_tz":240,"elapsed":234,"user":{"displayName":"Elijah Williams","userId":"12803059216149066698"}},"outputId":"e49727b8-e30e-42ca-80eb-aca524d45816"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.39973020553588867\n"]}]}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word's arousal is a measure of how exciting, interesting, attention-grabbing it is. In this notebook, we will assign an arousal to every word used in the texts, and then explore whether arousal influences how memorable sentences are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible Measures**\n",
    "\n",
    "1. [The Glasgow Norms](https://link.springer.com/article/10.3758/s13428-018-1099-3) - This study used 1000 in-person British people to score `5,000 words` on a scale from 1-9 (boring --> exciting).\n",
    "   * This is one of the best word pools out there because it takes into account that words have multiple meanings (*e.g.* a romantic date is much more exciting than calendar date)\n",
    "2. [Warriner, Kuperman, and Bysbaert Emotional Norms](https://link.springer.com/article/10.3758/s13428-012-0314-x) - This study used 2000 online raters to score `13,000 words` on a scale from 1-9 (boring --> exciting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (0) Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>rec_prob</th>\n",
       "      <th>concreteness_Brysbaert</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>word_length</th>\n",
       "      <th>word_frequency_cmx</th>\n",
       "      <th>word_concreteness_cmx</th>\n",
       "      <th>grade_level</th>\n",
       "      <th>input</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>passage_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Small and large abscesses may need to be treat...</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2.94</td>\n",
       "      <td>10</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.14</td>\n",
       "      <td>446.33</td>\n",
       "      <td>6.01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  rec_prob  \\\n",
       "0  Small and large abscesses may need to be treat...      0.84   \n",
       "\n",
       "   concreteness_Brysbaert  sentence_length  word_length  word_frequency_cmx  \\\n",
       "0                    2.94               10          1.5                2.14   \n",
       "\n",
       "   word_concreteness_cmx  grade_level  input  paragraph passage_id  \n",
       "0                 446.33         6.01      1          1         A1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "\n",
    "passages = [\"A1\", \"A2\", \"A3\", \"A4\", \"A5\"]  \n",
    "df = pd.DataFrame()\n",
    "\n",
    "for pas_id in passages:\n",
    "    pas_df = pd.read_excel(\"data\\ListA_TextProperties.xlsx\", sheet_name=pas_id).drop(columns=[\"max_arousal_Glasgow\", \"arousal_Glasgow\"], errors='ignore')\n",
    "    df = pd.concat([df, pas_df], ignore_index= True)\n",
    "\n",
    "df.head(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will be the dictionary we store each (word, arousal) pair in\n",
    "word_to_arousal_dictionary = {} \n",
    "\n",
    "## This will be the dictionary we store (word with stem, root word in word pool) pairs\n",
    "with open(\"data\\Arousal_equivalent_words.json\",\"r\") as dic:           \n",
    "    equivalence_dict = json.load(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Glasgow Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This word pool is interesting as it contains scores for ambiguous words. Let's load in the scores, and see what percent of words are covered by this word pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>arousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abattoir</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbey</td>\n",
       "      <td>3.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>3.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abdicate</td>\n",
       "      <td>4.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abdication</td>\n",
       "      <td>3.846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word arousal\n",
       "1    abattoir     4.2\n",
       "2       abbey   3.125\n",
       "3  abbreviate   3.273\n",
       "4    abdicate   4.194\n",
       "5  abdication   3.846"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (0) Load in word pool\n",
    "glasgow_arousal_df = pd.read_csv(\"data/Glasgow_Norms.csv\")\n",
    "glasgow_arousal_df = glasgow_arousal_df[[\"Words\", \"AROU\"]]\n",
    "glasgow_arousal_df = glasgow_arousal_df.rename(columns={\"Words\": \"word\", \"AROU\":\"arousal\"}).drop(0)\n",
    "\n",
    "glasgow_arousal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of Passage Words 406\n",
      "Number of Words Covered: 187\n",
      "Missing words: ['a', 'about', 'abrasions', 'abscess', 'abscesses', 'accessible', 'additional', 'airway', 'along', 'also', 'an', 'and', 'anesthetic', 'another', 'antibiotic', 'any', 'are', 'area', 'areas', 'around', 'as', 'ask', 'asking', 'assess', 'assessment', 'assessments', 'at', 'avpu', 'back', 'backward', 'be', 'because', 'before', 'bleeding', 'both', 'breathing', 'by', 'calculate', 'calculated', 'can', 'cause', 'caused', 'causes', 'chemical', 'chemicals', 'coma', 'compress', 'consciousness', 'considered', 'contacted', 'contamination', 'contusion', 'contusions', 'could', 'cravats', 'day', 'degree', 'different', 'differently', 'do', 'drainage', 'drapes', 'dressings', 'dry', 'each', 'eighteen', 'electrical', 'elevate', 'enough', 'environment', 'excess', 'exit', 'explosions', 'exposed', 'extended', 'extensive', 'eyeball', 'eyebrows', 'eyelashes', 'feeling', 'find', 'finding', 'five', 'flame', 'flush', 'for', 'found', 'fracture', 'from', 'further', 'gauze', 'get', 'getting', 'glasgow', 'healing', 'here', 'hoarseness', 'hot', 'how', 'if', 'immediately', 'in', 'incision', 'include', 'includes', 'increase', 'inflammation', 'inhalation', 'inject', 'insert', 'internal', 'into', 'involvement', 'is', 'it', 'jagged', 'less', 'level', 'look', 'looks', 'loss', 'may', 'medical', 'millimeters', 'minimize', 'movement', 'must', 'neurologic', 'next', 'nine', 'nines', 'no', 'normal', 'not', 'nothing', 'now', 'numbing', 'of', 'off', 'on', 'once', 'one', 'or', 'out', 'outline', 'percentage', 'piercing', 'place', 'possible', 'prep', 'prepare', 'problems', 'procedure', 'put', 'putting', 'ready', 'refer', 'remove', 'responses', 'restless', 'result', 'results', 'rinsed', 'roller', 'rule', 'saline', 'scrape', 'scrubbing', 'see', 'severity', 'sf600', 'should', 'side', 'sides', 'sign', 'signs', 'singed', 'six', 'size', 'so', 'soot', 'spinal', 'step', 'steps', 'sterile', 'such', 'surface', 'swab', 'tell', 'temperatures', 'than', 'the', 'their', 'them', 'there', 'thermal', 'these', 'they', 'thirty', 'this', 'three', 'times', 'tissue', 'to', 'treatment', 'turn', 'two', 'unresponsive', 'upper', 'vascular', 'wash', 'ways', 'were', 'when', 'wide', 'will', 'with', 'wounds', 'you']\n",
      "Ambiguous words (6 words): [('process', ['process', 'process (modify)', 'process (procedure)', 'process (understand)']), ('mark', ['mark', 'mark (grade)', 'mark (smudge)']), ('sharp', ['sharp', 'sharp (blade)', 'sharp (smart)']), ('scale', ['scale', 'scale (fish)', 'scale (magnitude)', 'scale (musical)', 'scale (weighing)']), ('lead', ['lead', 'lead (direct)', 'lead (heavy metal)', 'lead (leash)']), ('well', ['well', 'well (healthy)', 'well (water)'])]\n"
     ]
    }
   ],
   "source": [
    "# (0) Get a list of unique words used in each passage\n",
    "passage_words = \" \".join(df[\"sentence\"])                            # Passage string\n",
    "passage_words = passage_words.replace(\"%\",\" percent\")\n",
    "passage_words = passage_words.replace(\"18\",\" eighteen\")\n",
    "passage_words = passage_words.replace(\"36\",\" thirty six\")\n",
    "passage_words = passage_words.replace(\"9\",\" nine\")\n",
    "passage_words = passage_words.translate(str.maketrans('', '', string.punctuation))      # Removes punctuation\n",
    "passage_words = passage_words.lower().split()                       # Turns lower case and to list\n",
    "passage_words = list(set(passage_words))                            # Get unique words\n",
    "\n",
    "# (1) Find % of words immediately covered by word pool\n",
    "word_pool = glasgow_arousal_df[\"word\"].to_list()\n",
    "arousal_lst = glasgow_arousal_df[\"arousal\"].to_list()\n",
    "num_words_in_word_pool = 0\n",
    "not_in_pool = []\n",
    "words_with_multiple_meanings = []\n",
    "\n",
    "for passage_word in passage_words:                                          # For each word used in the passage, I want to check and see if it has a arousal scoring\n",
    "    scored_word_lst = []\n",
    "    origWord = passage_word                                                    # Save original word\n",
    "\n",
    "    for _ in range(2):                                                         # For the case:   passage_word -> root word -> word (meaning)\n",
    "        if passage_word in equivalence_dict:                                   # If the passage word needs to be stemmed, stem it.\n",
    "            passage_word = equivalence_dict[passage_word]\n",
    "\n",
    "\n",
    "    for scored_word, arousal in zip(word_pool, arousal_lst): \n",
    "        if scored_word.split(' (')[0] == passage_word or scored_word == passage_word:\n",
    "            scored_word_lst.append(scored_word)\n",
    "            word_to_arousal_dictionary[scored_word] = float(arousal)\n",
    "\n",
    "\n",
    "    if len(scored_word_lst) == 0:\n",
    "        not_in_pool.append(origWord)\n",
    "\n",
    "    elif len(scored_word_lst) >= 1:\n",
    "        num_words_in_word_pool += 1\n",
    "        if len(scored_word_lst) >= 2:\n",
    "            words_with_multiple_meanings.append((passage_word, scored_word_lst))\n",
    "\n",
    "    else:\n",
    "        not_in_pool.append(passage_word)\n",
    "\n",
    "\n",
    "percent = num_words_in_word_pool / len(passage_words)\n",
    "\n",
    "print(f\"Total # of Passage Words {len(passage_words)}\")\n",
    "print(f\"Number of Words Covered: {num_words_in_word_pool}\")\n",
    "print(f\"Missing words: {sorted(not_in_pool)}\")\n",
    "print(f\"Ambiguous words ({len(words_with_multiple_meanings)} words): {words_with_multiple_meanings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep the ratings for the 35% of words covered by this word pool, and try to cover the rest of the missing words using the Warriner, Cooper, Brysbaert ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) WKB Emotional Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>arousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abalone</td>\n",
       "      <td>2.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>4.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbey</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abdomen</td>\n",
       "      <td>3.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  arousal\n",
       "1      abalone     2.65\n",
       "2      abandon     3.73\n",
       "3  abandonment     4.95\n",
       "4        abbey     2.20\n",
       "5      abdomen     3.68"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (0) Load in Word Pool\n",
    "wkb_arousal_df = pd.read_excel(\"data\\WKB_Emotion_Norms.xlsx\")\n",
    "wkb_arousal_df = wkb_arousal_df[[\"Word\", \"A.Mean.Sum\"]]\n",
    "wkb_arousal_df = wkb_arousal_df.rename(columns={\"Word\": \"word\", \"A.Mean.Sum\":\"arousal\"}).drop(0)\n",
    "\n",
    "wkb_arousal_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words covered: 154\n",
      "Covered Words: ['abscess', 'abscess', 'accessible', 'additional', 'airway', 'anesthetic', 'antibiotic', 'area', 'area', 'ask', 'ask', 'assess', 'assessment', 'assessment', 'back', 'back', 'be', 'bleed', 'breath', 'calculate', 'calculate', 'can', 'cause', 'cause', 'cause', 'chemical', 'chemical', 'coma', 'compress', 'consciousness', 'consider', 'contact', 'contamination', 'contusion', 'contusion', 'day', 'degree', 'different', 'different', 'do', 'drainage', 'drape', 'dry', 'eighteen', 'electrical', 'elevate', 'environment', 'excess', 'exit', 'explosion', 'expose', 'extend', 'extensive', 'eyeball', 'eyebrow', 'eyelash', 'feeling', 'find', 'find', 'find', 'five', 'flame', 'flush', 'fracture', 'gauze', 'get', 'get', 'healing', 'hoarse', 'hot', 'immediate', 'incision', 'include', 'include', 'increase', 'inflammation', 'inhalation', 'inject', 'insert', 'internal', 'involvement', 'jagged', 'level', 'look', 'look', 'loss', 'may', 'medical', 'millimeter', 'minimize', 'movement', 'must', 'neurological', 'nine', 'nine', 'normal', 'numb', 'one', 'outline', 'penetrate', 'percentage', 'place', 'possible', 'prep', 'prepare', 'problem', 'procedure', 'put', 'put', 'ready', 'refer', 'remove', 'response', 'restless', 'result', 'result', 'rinse', 'roller', 'rule', 'saline', 'scrape', 'scrub', 'see', 'severity', 'side', 'side', 'sign', 'sign', 'six', 'size', 'soot', 'spinal', 'step', 'steps', 'sterile', 'surface', 'swab', 'tell', 'temperature', 'thermal', 'thirty', 'three', 'times', 'tissue', 'treatment', 'turn', 'two', 'unresponsive', 'upper', 'vascular', 'wash', 'wide', 'will', 'wounds']\n",
      "Number of missing words: 61\n",
      "Missing words: ['a', 'about', 'along', 'also', 'an', 'and', 'another', 'any', 'are', 'around', 'as', 'at', 'avpu', 'because', 'before', 'both', 'by', 'could', 'each', 'enough', 'for', 'from', 'further', 'glasgow', 'here', 'how', 'if', 'in', 'into', 'is', 'it', 'less', 'next', 'no', 'not', 'nothing', 'now', 'of', 'off', 'on', 'once', 'or', 'out', 'sf600', 'should', 'so', 'such', 'than', 'the', 'their', 'them', 'there', 'these', 'they', 'this', 'to', 'ways', 'were', 'when', 'with', 'you']\n"
     ]
    }
   ],
   "source": [
    "# (1) Find the number of words covered by this word pool, along with the words that are still missing\n",
    "scored_words = wkb_arousal_df.word.to_list()\n",
    "arousal_lst = wkb_arousal_df.arousal.to_list()\n",
    "covered_by_pool_lst = []\n",
    "\n",
    "for missing_word in not_in_pool:                                # For each word that's missing\n",
    "\n",
    "\n",
    "    if missing_word in equivalence_dict:                        # Check if there is equivalent word in word pool\n",
    "        not_in_pool.remove(missing_word)\n",
    "        missing_word = equivalence_dict[missing_word]\n",
    "\n",
    "    if missing_word in scored_words:\n",
    "        index =  scored_words.index(missing_word)\n",
    "        covered_by_pool_lst.append(missing_word)\n",
    "        word_to_arousal_dictionary[missing_word] = float(arousal_lst[index])\n",
    "        not_in_pool = [word for word in not_in_pool if word != missing_word]\n",
    "\n",
    "    # for scored_word, arousal in zip(scored_words, arousal_lst): # Check if word in word pool\n",
    "    #     if missing_word == scored_word:\n",
    "    #         covered_by_pool_lst.append(missing_word)\n",
    "    #         word_to_arousal_dictionary[scored_word] = float(arousal)\n",
    "    #         not_in_pool.remove(missing_word)\n",
    "    \n",
    "\n",
    "print(f\"Number of words covered: {len(covered_by_pool_lst)}\")\n",
    "print(f\"Covered Words: {sorted(covered_by_pool_lst)}\")\n",
    "print(f\"Number of missing words: {len(not_in_pool)}\")\n",
    "print(f\"Missing words: {sorted(not_in_pool)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our word pool so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steam': 4.419, 'object (thing)': 3.118, 'note': 4.065, 'promote': 5.516, 'symptom': 4.226, 'wrap': 4.059, 'treat': 6.879, 'forceful': 5.912, 'other': 2.742, 'wound (cut)': 5.206, 'opening': 5.0, 'roll (turn over)': 4.364, 'injury': 5.091, 'danger': 7.147, 'warm': 5.857, 'kind (type)': 3.758, 'verbal': 4.706, 'process': 3.471, 'process (modify)': 3.571, 'process (procedure)': 3.088, 'process (understand)': 3.97, 'electricity': 5.849, 'give': 4.625, 'clear': 4.667, 'scalp': 3.636, 'measure': 4.125, 'second (position)': 4.152, 'killer': 6.303, 'near': 4.546, 'eye': 3.886, 'record (information)': 3.6, 'patient (medical)': 4.324, 'make': 4.765, 'cover': 4.226, 'type (category)': 3.343, 'apply': 3.971, 'infection': 4.471, 'carry': 3.563, 'motor': 3.941, 'hard (not soft)': 5.121, 'small': 3.424, 'slight (little)': 3.057, 'pain': 5.515, 'severe': 5.206, 'check': 3.546, 'stop': 4.355, 'safe': 4.912, 'test': 4.242, 'high': 5.879, 'black': 3.657, 'sure': 4.914, 'victim': 5.486, 'entry': 4.594, 'sickness': 3.794, 'like (similar to)': 3.382, 'soak': 4.0, 'bandage': 4.2, 'combustion': 5.971, 'prior': 2.781, 'open': 5.8, 'date (calendar)': 3.394, 'product': 3.829, 'help': 4.563, 'body': 5.212, 'clean': 5.147, 'away': 3.594, 'follow': 3.906, 'equipment': 3.853, 'all': 3.625, 'finish': 6.086, 'smoke': 5.212, 'fold': 3.323, 'syringe': 5.469, 'lid': 2.677, 'damage': 4.618, 'fluid': 4.5, 'large': 5.75, 'up': 5.229, 'mark': 3.667, 'mark (grade)': 5.143, 'mark (smudge)': 3.152, 'slow': 2.636, 'down (direction)': 3.188, 'health': 5.406, 'skin': 5.774, 'mouth': 6.194, 'flow': 4.057, 'cotton': 3.559, 'burnt': 5.194, 'poke': 4.938, 'last': 3.29, 'need': 5.344, 'aid': 5.219, 'hold (grasp)': 4.765, 'drop (fall)': 4.469, 'sharp': 6.29, 'sharp (blade)': 5.371, 'sharp (smart)': 5.515, 'first': 5.515, 'brain': 5.936, 'use': 4.364, 'officer': 4.0, 'name': 4.286, 'care': 5.294, 'time': 3.903, 'complete': 5.857, 'scale': 2.853, 'scale (fish)': 3.121, 'scale (magnitude)': 4.5, 'scale (musical)': 5.118, 'scale (weighing)': 3.588, 'percent': 3.265, 'low': 3.353, 'ice': 5.212, 'sensation': 7.677, 'over (above)': 4.152, 'towel': 3.419, 'cream': 4.515, 'prevent': 3.818, 'start': 5.129, 'oxygen': 4.941, 'neck': 5.433, 'deformed': 4.667, 'residue': 3.091, 'gentle': 5.147, 'careful': 4.152, 'secure': 4.371, 'limb (body)': 3.824, 'lead': 4.25, 'lead (direct)': 4.941, 'lead (heavy metal)': 3.2, 'lead (leash)': 4.091, 'close (near)': 4.531, 'have': 4.438, 'rate': 4.061, 'everything': 5.735, 'eyes': 5.441, 'easy': 4.914, 'impact': 5.516, 'describe': 4.177, 'loud': 5.588, 'pull (drag)': 3.849, 'current': 4.645, 'strike (hit)': 5.265, 'burn': 4.818, 'block': 3.303, 'lightning': 7.032, 'needle': 4.829, 'soft': 5.029, 'method': 3.441, 'begin': 4.941, 'damaged': 4.371, 'proper': 3.839, 'paralysis': 4.441, 'casualty': 4.152, 'pus': 3.933, 'seek': 4.824, 'stuck': 4.394, 'host': 4.424, 'amount': 3.647, 'hit (beat on)': 4.879, 'well': 4.576, 'well (healthy)': 5.114, 'well (water)': 3.758, 'endure': 4.059, 'liquid': 4.177, 'cut (wound)': 5.406, 'support': 4.758, 'foreign': 5.091, 'history': 3.849, 'water': 4.857, 'blood': 5.406, 'hand': 3.794, 'under': 4.2, 'take': 4.265, 'strain': 4.424, 'after': 4.382, 'head (body)': 4.824, 'get': 3.67, 'find': 3.52, 'ask': 3.48, 'environment': 3.45, 'vascular': 4.55, 'contact': 3.33, 'incision': 4.9, 'problem': 4.81, 'spinal': 4.35, 'surface': 3.04, 'additional': 3.61, 'back': 2.59, 'chemical': 3.41, 'bleed': 5.0, 'hoarse': 3.17, 'internal': 3.67, 'excess': 5.42, 'eyeball': 3.86, 'expose': 5.89, 'calculate': 3.18, 'thermal': 3.48, 'involvement': 2.95, 'thirty': 3.57, 'tissue': 3.1, 'side': 3.14, 'rinse': 2.76, 'normal': 2.29, 'place': 3.52, 'assessment': 2.75, 'eyelash': 2.61, 'cause': 3.48, 'area': 2.19, 'sterile': 4.47, 'contusion': 5.95, 'turn': 3.48, 'be': 3.43, 'electrical': 6.0, 'different': 3.95, 'steps': 3.0, 'put': 3.86, 'outline': 2.62, 'nine': 3.83, 'ready': 3.9, 'healing': 3.67, 'scrape': 4.5, 'two': 3.33, 'swab': 2.56, 'coma': 3.05, 'accessible': 3.2, 'drainage': 3.52, 'wounds': 4.0, 'compress': 5.0, 'explosion': 6.35, 'may': 3.19, 'feeling': 3.86, 'consciousness': 3.33, 'eighteen': 3.83, 'sign': 3.19, 'five': 3.55, 'airway': 4.0, 'temperature': 4.86, 'restless': 4.05, 'eyebrow': 2.91, 'antibiotic': 4.05, 'can': 3.14, 'anesthetic': 3.23, 'prep': 3.19, 'medical': 4.05, 'do': 3.67, 'immediate': 4.64, 'result': 4.14, 'tell': 3.86, 'minimize': 4.19, 'look': 3.76, 'degree': 4.1, 'breath': 2.35, 'inflammation': 4.46, 'level': 2.15, 'extensive': 4.75, 'roller': 3.15, 'prepare': 4.15, 'must': 4.1, 'day': 3.62, 'hot': 4.76, 'jagged': 6.05, 'will': 2.9, 'possible': 3.71, 'movement': 4.0, 'saline': 3.2, 'unresponsive': 3.94, 'rule': 3.67, 'procedure': 4.15, 'increase': 5.06, 'elevate': 4.65, 'abscess': 4.0, 'percentage': 2.96, 'upper': 3.52, 'exit': 3.18, 'dry': 3.8, 'treatment': 4.47, 'one': 2.67, 'loss': 5.2, 'soot': 4.65, 'include': 3.05, 'inject': 4.14, 'inhalation': 3.33, 'numb': 2.5, 'scrub': 3.95, 'insert': 3.68, 'refer': 4.09, 'wash': 3.7, 'six': 3.18, 'wide': 3.05, 'contamination': 6.71, 'times': 3.19, 'flame': 6.6, 'millimeter': 3.62, 'size': 3.14, 'step': 3.48, 'severity': 5.48, 'assess': 3.45, 'neurological': 4.55, 'fracture': 5.27, 'remove': 3.11, 'three': 3.38, 'gauze': 3.2, 'drape': 3.29, 'see': 3.9, 'penetrate': 6.08, 'response': 3.56, 'consider': 2.71, 'extend': 3.62, 'flush': 4.35}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_arousal_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Score Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def arousal(item_string: str, word_pool: dict, equivalence_dict: dict = {}):\n",
    "    \"\"\"Returns the average arousal  of the word in a text\n",
    "\n",
    "    Args:\n",
    "        item_string (str): \n",
    "            A sentence to compute arousal of\n",
    "        \n",
    "        word_pool (dict[str, int]):\n",
    "            Dictionary of (word, arousal) pairs\n",
    "\n",
    "        equivalence_dict (dict[str, str]):\n",
    "            Dictionary mapping words to equivalent words in the word pool\n",
    "\n",
    "    Returns:\n",
    "        max_arousal (int): \n",
    "            maximum arousal of the words in the sentence\n",
    "\n",
    "        missing_words (list): \n",
    "            List of words not in word pool\n",
    "    \"\"\"\n",
    "\n",
    "    # (0) Clean up string -- change '%' -> 'percent', remove punctuation, lowercase letters\n",
    "    item_string = item_string.replace(\"%\",\" percent\")\n",
    "    item_string = item_string.translate(str.maketrans('', '', string.punctuation))\n",
    "    item_string = item_string.lower()\n",
    "\n",
    "    # (1) Compute arousal of each word in string\n",
    "    arousal_lst = []\n",
    "    missing_words = []\n",
    "\n",
    "    for word in item_string.split():\n",
    "        if word in equivalence_dict:\n",
    "            word = equivalence_dict[word]\n",
    "            \n",
    "        if word in word_pool: \n",
    "            arousal_lst.append(word_pool[word])\n",
    "        else:\n",
    "            missing_words.append(word)\n",
    "\n",
    "    if arousal_lst != []:\n",
    "        avg_arousal = sum(arousal_lst)/len(arousal_lst)      # watch out for division by zero!\n",
    "    else:\n",
    "        avg_arousal = np.nan\n",
    "\n",
    "    return avg_arousal, missing_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# (1) Load in word pool and dictionary equivalent words\n",
    "word_pool = word_to_arousal_dictionary\n",
    "\n",
    "with open(\"data\\Arousal_equivalent_words.json\",\"r\") as dic:           \n",
    "    equivalence_dict = json.load(dic)\n",
    "    \n",
    "# (2) Assign an arousal to each sentence and save results\n",
    "missing_words = []\n",
    "with pd.ExcelWriter(\"data/ListA_TextProperties.xlsx\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    for pas_id in passages:\n",
    "        pas_df = df.query(f\"passage_id == '{pas_id}'\")\n",
    "        arousal_column =  [arousal(sentence, word_pool, equivalence_dict)[0] for sentence in pas_df.sentence]\n",
    "        missing_words.append([arousal(sentence, word_pool, equivalence_dict)[1] for sentence in pas_df.sentence])\n",
    "        pas_df[\"arousal_Glasgow\"] = arousal_column\n",
    "        pas_df = pas_df[[\"sentence\", \"rec_prob\", \"arousal_Glasgow\"] + pas_df.columns[2:-1].to_list()]    \n",
    "        pas_df.round(2).to_excel(writer, sheet_name=pas_id, index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) Check Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check whether these arousal rating align with intution, and that the fact that words have multiple meanings doesn't mess us the scoring too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Arousal</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Probability of Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>sensation</td>\n",
       "      <td>7.677</td>\n",
       "      <td>Test the sensation and movement around the wou...</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>dangers.</td>\n",
       "      <td>7.147</td>\n",
       "      <td>Secure a safe environment, clear of any close ...</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>lightning</td>\n",
       "      <td>7.032</td>\n",
       "      <td>Victims of lightning strikes may need extended...</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>lightning</td>\n",
       "      <td>7.032</td>\n",
       "      <td>Explosions and lightning may cause burn and ad...</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>treating</td>\n",
       "      <td>6.879</td>\n",
       "      <td>Make sure everything is ready before treating ...</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>treated</td>\n",
       "      <td>6.879</td>\n",
       "      <td>Small and large abscesses may need to be treat...</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>treat</td>\n",
       "      <td>6.879</td>\n",
       "      <td>Once the burn process is stopped, begin to tre...</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>contamination</td>\n",
       "      <td>6.710</td>\n",
       "      <td>Completing this step also prevents further con...</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>flame,</td>\n",
       "      <td>6.600</td>\n",
       "      <td>Possible causes include open flame, hot liquid...</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Explosions</td>\n",
       "      <td>6.350</td>\n",
       "      <td>Explosions and lightning may cause burn and ad...</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word  Arousal  \\\n",
       "869      sensation    7.677   \n",
       "610       dangers.    7.147   \n",
       "270      lightning    7.032   \n",
       "224      lightning    7.032   \n",
       "59        treating    6.879   \n",
       "8          treated    6.879   \n",
       "296          treat    6.879   \n",
       "566  contamination    6.710   \n",
       "216         flame,    6.600   \n",
       "222     Explosions    6.350   \n",
       "\n",
       "                                              Sentence  Probability of Recall  \n",
       "869  Test the sensation and movement around the wou...                   0.11  \n",
       "610  Secure a safe environment, clear of any close ...                   0.38  \n",
       "270  Victims of lightning strikes may need extended...                   0.33  \n",
       "224  Explosions and lightning may cause burn and ad...                   0.54  \n",
       "59   Make sure everything is ready before treating ...                   0.24  \n",
       "8    Small and large abscesses may need to be treat...                   0.84  \n",
       "296  Once the burn process is stopped, begin to tre...                   0.38  \n",
       "566  Completing this step also prevents further con...                   0.33  \n",
       "216  Possible causes include open flame, hot liquid...                   0.72  \n",
       "222  Explosions and lightning may cause burn and ad...                   0.54  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1) Let's create a dataframe that associates each word, with a sentence, and an arousal\n",
    "\n",
    "validation_df = pd.DataFrame(columns=[\"Word\", \"Arousal\", \"Sentence\",\"Probability of Recall\"])\n",
    "\n",
    "for i, row in df.iterrows():        # For each sentence\n",
    "    sentence = row.sentence\n",
    "    for word in sentence.split():   # For each word in a sentence\n",
    "        validation_df.loc[len(validation_df)] = [word, arousal( word, word_pool, equivalence_dict)[0] , sentence, row.rec_prob]\n",
    "\n",
    "validation_df = validation_df.sort_values(\"Arousal\", ascending=False).dropna(subset=[\"Arousal\"])\n",
    "validation_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like super exciting words aren't particularly well remembered. Let's investigate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>rec_prob</th>\n",
       "      <th>arousal_Glasgow</th>\n",
       "      <th>concreteness_Brysbaert</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>word_length</th>\n",
       "      <th>word_frequency_cmx</th>\n",
       "      <th>word_concreteness_cmx</th>\n",
       "      <th>grade_level</th>\n",
       "      <th>input</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>passage_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Make sure everything is ready before treating ...</td>\n",
       "      <td>0.24</td>\n",
       "      <td>5.09</td>\n",
       "      <td>2.23</td>\n",
       "      <td>9</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.71</td>\n",
       "      <td>338.2</td>\n",
       "      <td>7.59</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  rec_prob  \\\n",
       "5  Make sure everything is ready before treating ...      0.24   \n",
       "\n",
       "   arousal_Glasgow  concreteness_Brysbaert  sentence_length  word_length  \\\n",
       "5             5.09                    2.23                9         1.67   \n",
       "\n",
       "   word_frequency_cmx  word_concreteness_cmx  grade_level  input  paragraph  \\\n",
       "5                2.71                  338.2         7.59      6          2   \n",
       "\n",
       "  passage_id  \n",
       "5         A1  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (0) Load in Data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "passages = [\"A1\", \"A2\", \"A3\", \"A4\", \"A5\"]  \n",
    "df = pd.DataFrame()\n",
    "\n",
    "for pas_id in passages:\n",
    "    pas_df = pd.read_excel(\"data\\ListA_TextProperties.xlsx\", sheet_name=pas_id)\n",
    "    df = pd.concat([df, pas_df], ignore_index= True)\n",
    "\n",
    "df = df.sort_values(\"arousal_Glasgow\", ascending=False)\n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Recall of  Sentences: 0.4886\n",
      "Mean Recall of Exciting Sentences: 0.4364\n",
      "Mean Recall of Boring Sentences: 0.5456\n"
     ]
    }
   ],
   "source": [
    "upper_percentile = np.percentile(df.arousal_Glasgow, q=75)\n",
    "lower_percentile = np.percentile(df.arousal_Glasgow, q=25)\n",
    "arousing_sentences = df.query(f\"arousal_Glasgow > {upper_percentile}\")\n",
    "boring_sentences = df.query(f\"arousal_Glasgow < {lower_percentile}\")\n",
    "print(\"Mean Recall of  Sentences:\", df.rec_prob.mean())\n",
    "print(\"Mean Recall of Exciting Sentences:\", arousing_sentences.rec_prob.mean())\n",
    "print(\"Mean Recall of Boring Sentences:\", boring_sentences.rec_prob.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
